# scripts/train_lda.py

import os
import re
import joblib
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# =========================
# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# =========================

# ThÆ° má»¥c gá»‘c project (D:\hanhvinguoidung)
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# ÄÆ°á»ng dáº«n file dá»¯ liá»‡u
# ğŸ‘‰ Náº¿u file dá»¯ liá»‡u cá»§a báº¡n KHÃ”NG tÃªn lÃ  tweets.csv thÃ¬ sá»­a á»Ÿ Ä‘Ã¢y
DATA_PATH = os.path.join(PROJECT_ROOT, "data", "raw", "tweets.csv")

# ThÆ° má»¥c lÆ°u model (Ä‘Ãºng theo yÃªu cáº§u: D:\hanhvinguoidung\models)
OUT_DIR = os.path.join(PROJECT_ROOT, "models")
os.makedirs(OUT_DIR, exist_ok=True)


# =========================
# HÃ€M TIá»€N Xá»¬ LÃ TEXT
# =========================

def clean_text(text: str) -> str:
    """
    Tiá»n xá»­ lÃ½ má»™t chuá»—i vÄƒn báº£n:
    - chuyá»ƒn vá» chá»¯ thÆ°á»ng
    - bá» URL, @tag, #hashtag
    - bá» kÃ½ tá»± Ä‘áº·c biá»‡t
    - bá» khoáº£ng tráº¯ng thá»«a
    """
    if not isinstance(text, str):
        text = str(text)

    text = text.lower()

    # Bá» URL
    text = re.sub(r"http\S+|www\.\S+", " ", text)

    # Bá» @username
    text = re.sub(r"@\w+", " ", text)

    # Bá» dáº¥u #
    text = text.replace("#", " ")

    # Bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ chá»¯ + sá»‘ + tiáº¿ng Viá»‡t
    text = re.sub(
        r"[^0-9a-zA-ZÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­"
        r"Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹"
        r"Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£"
        r"ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±"
        r"Ã½á»³á»·á»¹á»µÄ‘\s]",
        " ",
        text,
    )

    # Bá» khoáº£ng tráº¯ng dÆ°
    text = re.sub(r"\s+", " ", text).strip()
    return text


def preprocess_series(series: pd.Series):
    """
    Nháº­n vÃ o pandas Series chá»©a text â†’ tráº£ vá» list text Ä‘Ã£ xá»­ lÃ½.
    """
    series = series.dropna().astype(str)
    processed = series.apply(clean_text)
    processed = processed[processed.str.len() > 0]
    return processed.tolist()


# =========================
# HÃ€M HUáº¤N LUYá»†N LDA
# =========================

def train_lda(texts, n_topics=6, max_features=2000):
    """
    Huáº¥n luyá»‡n mÃ´ hÃ¬nh LDA tá»« list chuá»—i 'texts'.
    Tráº£ vá»:
      - mÃ´ hÃ¬nh LDA
      - vectorizer (CountVectorizer)
    """
    if not isinstance(texts, (list, tuple)):
        raise ValueError("texts pháº£i lÃ  list/tuple cÃ¡c chuá»—i vÄƒn báº£n Ä‘Ã£ tiá»n xá»­ lÃ½.")

    vectorizer = CountVectorizer(
        max_df=0.95,
        min_df=2,
        max_features=max_features,
        stop_words="english",  # náº¿u dá»¯ liá»‡u thuáº§n Viá»‡t lÃ  chÃ­nh, cÃ³ thá»ƒ Ä‘á»•i None
    )

    X = vectorizer.fit_transform(texts)

    n_components = min(n_topics, X.shape[0])  # trÃ¡nh n_topics > sá»‘ máº«u
    lda = LatentDirichletAllocation(
        n_components=n_components,
        learning_method="batch",
        random_state=42,
    )

    lda.fit(X)
    return lda, vectorizer


# =========================
# Tá»° Äá»˜NG ÄOÃN Cá»˜T TEXT
# =========================

def detect_text_col(df: pd.DataFrame):
    """
    Tá»± Ä‘á»™ng Ä‘oÃ¡n cá»™t vÄƒn báº£n trong DataFrame.
    Æ¯u tiÃªn cÃ¡c tÃªn quen thuá»™c, náº¿u khÃ´ng cÃ³ thÃ¬ chá»n cá»™t kiá»ƒu object Ä‘áº§u tiÃªn.
    """
    candidates = ["text", "tweet", "content", "message", "body"]
    for c in candidates:
        if c in df.columns:
            return c

    for c in df.columns:
        if df[c].dtype == object:
            return c

    return None


# =========================
# MAIN
# =========================

if __name__ == "__main__":
    # 1. Kiá»ƒm tra dá»¯ liá»‡u
    if not os.path.exists(DATA_PATH):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {DATA_PATH}")
        print("ğŸ‘‰ HÃ£y Ä‘áº·t file CSV vÃ o data/raw/tweets.csv hoáº·c sá»­a láº¡i DATA_PATH trong train_lda.py.")
        raise SystemExit(1)

    print(f"âœ… Äang Ä‘á»c dá»¯ liá»‡u tá»«: {DATA_PATH}")
    df = pd.read_csv(DATA_PATH)

    # 2. XÃ¡c Ä‘á»‹nh cá»™t text
    text_col = detect_text_col(df)
    if text_col is None:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t vÄƒn báº£n trong CSV.")
        print("   HÃ£y kiá»ƒm tra láº¡i tÃªn cá»™t vÃ  cáº­p nháº­t hÃ m detect_text_col().")
        raise SystemExit(1)

    print(f"âœ… ÄÃ£ chá»n cá»™t vÄƒn báº£n: '{text_col}'")

    # 3. Tiá»n xá»­ lÃ½
    texts = preprocess_series(df[text_col])
    print(f"âœ… ÄÃ£ tiá»n xá»­ lÃ½ {len(texts)} dÃ²ng vÄƒn báº£n.")

    if len(texts) < 10:
        print("âš ï¸ Dá»¯ liá»‡u sau tiá»n xá»­ lÃ½ quÃ¡ Ã­t (< 10 dÃ²ng), LDA cÃ³ thá»ƒ khÃ´ng á»•n.")

    # 4. Huáº¥n luyá»‡n LDA
    print("â³ Äang huáº¥n luyá»‡n LDA...")
    lda, vect = train_lda(texts, n_topics=6, max_features=2000)
    print("âœ… Huáº¥n luyá»‡n xong LDA.")

    # 5. LÆ°u model
    lda_path = os.path.join(OUT_DIR, "lda_model.pkl")
    vec_path = os.path.join(OUT_DIR, "vectorizer.pkl")

    joblib.dump(lda, lda_path)
    joblib.dump(vect, vec_path)

    print("âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh:")
    print("   LDA model  ->", lda_path)
    print("   Vectorizer ->", vec_path)
