# ======================================================
# app.py 
# ======================================================

import re
from pathlib import Path
import pandas as pd
import numpy as np
import streamlit as st
import joblib

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.metrics import classification_report, accuracy_score

# ---------- CONFIG ----------
st.set_page_config(
    page_title="PhÃ¢n tÃ­ch hÃ nh vi ngÆ°á»i dÃ¹ng trÃªn MXH",
    layout="wide"
)

MODEL_DIR = Path("models")
MODEL_DIR.mkdir(exist_ok=True)

# ---------- INIT SESSION STATE ----------
if "TEXT_COL" not in st.session_state:
    st.session_state.TEXT_COL = None
if "comments_input" not in st.session_state:
    st.session_state.comments_input = ""
if "comment_results" not in st.session_state:
    st.session_state.comment_results = []
if "df_processed" not in st.session_state:
    st.session_state.df_processed = None
if "topic_result" not in st.session_state:
    st.session_state.topic_result = None

# ---------- STOPWORDS & LEXICON ----------
VI_STOPWORDS = set("""
lÃ  thÃ¬ mÃ  ráº¥t quÃ¡ nÃ y kia Ä‘Ã³ má»™t cÃ¡c Ä‘Æ°á»£c cho vá»›i cÃ³ nhá»¯ng
Ä‘ang Ä‘Ã£ sáº½ cÅ©ng vÃ¬ táº¡i tá»« khi náº¿u nhÆ°ng nÃªn
""".split())

NEGATION_WORDS = {"khÃ´ng", "cháº³ng"}
SENTIMENT_WORDS = {
    "tá»‘t","xáº¥u","tá»‡","á»•n","hay","dá»Ÿ",
    "Ä‘áº¹p","kinh","chÃ¡n","ghÃª","ok","kÃ©m"
}

# ---------- REGEX ----------
URL_RE = re.compile(r'https?://\S+|www\.\S+')
MENTION_RE = re.compile(r'@\w+')
HASHTAG_RE = re.compile(r'#(\w+)')
NON_ALPHA_RE = re.compile(
    r'[^a-z0-9Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­'
    r'Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡'
    r'Ã­Ã¬á»‰Ä©á»‹'
    r'Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£'
    r'ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±'
    r'Ã½á»³á»·á»¹á»µÄ‘ ]'
)
MULTI_SPACE_RE = re.compile(r'\s+')

# ---------- CLEAN TEXT (TRAIN â€“ GIá»® NGUYÃŠN LOGIC Gá»C) ----------
def clean_text_sentiment(s):
    if pd.isna(s):
        return ""
    s = str(s).lower()
    s = URL_RE.sub(" ", s)
    s = MENTION_RE.sub(" ", s)
    s = HASHTAG_RE.sub(r'\1', s)
    s = NON_ALPHA_RE.sub(" ", s)
    s = MULTI_SPACE_RE.sub(" ", s).strip()

    words = s.split()
    output, negate = [], False

    for w in words:
        if w in NEGATION_WORDS:
            negate = True
            continue
        if negate and w in SENTIMENT_WORDS:
            output.append("NEG_" + w)
        elif w not in VI_STOPWORDS and len(w) > 1:
            output.append(w)
        negate = False

    return " ".join(output)

# ---------- CLEAN TEXT (INFERENCE â€“ á»”N Äá»ŠNH) ----------
def clean_text_inference(s):
    if pd.isna(s):
        return ""
    s = str(s).lower()
    s = URL_RE.sub(" ", s)
    s = MENTION_RE.sub(" ", s)
    s = HASHTAG_RE.sub(r'\1', s)
    s = NON_ALPHA_RE.sub(" ", s)
    return MULTI_SPACE_RE.sub(" ", s).strip()

def clean_text_topic(s):
    return clean_text_inference(s)

# ---------- LABEL ----------
def sentiment_bucket(label):
    label = str(label).lower()
    if "neg" in label:
        return "Negative"
    if "pos" in label:
        return "Positive"
    return None

# ---------- CONFIDENCE ----------
def calibrated_confidence(p):
    return np.clip(0.5 + (p - 0.5) * 1.6, 0, 1)

def confidence_level(conf):
    if conf >= 75:
        return "Cao"
    if conf >= 60:
        return "Trung bÃ¬nh"
    return "Tháº¥p"

def confidence_icon(level):
    return {"Cao": "ğŸŸ¢", "Trung bÃ¬nh": "ğŸŸ¡", "Tháº¥p": "ğŸ”´"}[level]

def explain_comment(sentiment, conf):
    lvl = confidence_level(conf)
    if sentiment == "Positive":
        return f"Cáº£m xÃºc tÃ­ch cá»±c ({lvl}) â€“ ngÆ°á»i dÃ¹ng cÃ³ xu hÆ°á»›ng hÃ i lÃ²ng."
    return f"Cáº£m xÃºc tiÃªu cá»±c ({lvl}) â€“ ngÆ°á»i dÃ¹ng cÃ³ dáº¥u hiá»‡u khÃ´ng hÃ i lÃ²ng."

# ---------- LOAD DATA ----------
@st.cache_data
def load_data(file):
    try:
        return pd.read_csv(file)
    except:
        return pd.read_csv(file, encoding="latin-1")

# ---------- SIDEBAR ----------
st.sidebar.title("ğŸ“Š á»¨ng dá»¥ng MXH")
uploaded = st.sidebar.file_uploader("ğŸ“ Upload CSV", type=["csv"])
if not uploaded:
    st.stop()

df = load_data(uploaded)

page = st.sidebar.radio(
    "ğŸ“Œ Chá»©c nÄƒng",
    [
        "ğŸ“ Xem dá»¯ liá»‡u",
        "ğŸ§  Huáº¥n luyá»‡n mÃ´ hÃ¬nh",
        "ğŸ’¬ PhÃ¢n tÃ­ch comment",
        "ğŸ›’ PhÃ¢n tÃ­ch theo sáº£n pháº©m",
        "ğŸ§© Topic Modeling"
    ]
)

# ======================================================
# ğŸ“ VIEW DATA
# ======================================================
if page == "ğŸ“ Xem dá»¯ liá»‡u":
    st.title("ğŸ“ Dá»¯ liá»‡u Ä‘áº§u vÃ o")

    show_processed = st.checkbox(
        "ğŸ” Hiá»ƒn thá»‹ dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ (clean_text)",
        value=st.session_state.df_processed is not None
    )

    if show_processed:
        if st.session_state.TEXT_COL is None:
            st.warning("Cáº§n huáº¥n luyá»‡n model trÆ°á»›c")
        else:
            if st.session_state.df_processed is None:
                tmp = df.copy()
                tmp["clean_text"] = tmp[
                    st.session_state.TEXT_COL
                ].apply(clean_text_sentiment)
                st.session_state.df_processed = tmp

            st.dataframe(
                st.session_state.df_processed[
                    [st.session_state.TEXT_COL, "clean_text"]
                ].head(50)
            )
    else:
        st.dataframe(df.head(50))

# ======================================================
# ğŸ§  TRAIN SENTIMENT
# ======================================================
elif page == "ğŸ§  Huáº¥n luyá»‡n mÃ´ hÃ¬nh":
    st.title("ğŸ§  Huáº¥n luyá»‡n Sentiment")

    text_col = st.selectbox("ğŸ“„ Cá»™t ná»™i dung", df.columns)
    label_col = st.selectbox("ğŸ·ï¸ Cá»™t nhÃ£n", df.columns)

    if st.button("â–¶ï¸ Huáº¥n luyá»‡n"):
        st.session_state.TEXT_COL = text_col
        st.session_state.df_processed = None

        data = df[[text_col, label_col]].dropna().copy()
        data["clean"] = data[text_col].apply(clean_text_sentiment)
        data["label"] = data[label_col].apply(sentiment_bucket)
        data = data.dropna()

        Xtr, Xte, ytr, yte = train_test_split(
            data["clean"],
            data["label"],
            test_size=0.2,
            stratify=data["label"],
            random_state=42
        )

        model = Pipeline([
            ("features", FeatureUnion([
                ("word", TfidfVectorizer(
                    ngram_range=(1, 2),
                    max_features=14000,
                    min_df=2,
                    max_df=0.9,
                    sublinear_tf=True
                )),
                ("char", TfidfVectorizer(
                    analyzer="char",
                    ngram_range=(3, 4),
                    max_features=2000
                ))
            ], transformer_weights={"word": 0.85, "char": 0.15})),
            ("clf", LogisticRegression(
                max_iter=500,
                solver="liblinear",
                class_weight="balanced"
            ))
        ])

        model.fit(Xtr, ytr)
        acc = accuracy_score(yte, model.predict(Xte))

        st.success(f"Accuracy: {acc:.2%}")
        st.text(classification_report(yte, model.predict(Xte)))

        joblib.dump(model, MODEL_DIR / "sentiment_model.joblib")

# ======================================================
# ğŸ’¬ COMMENT ANALYSIS
# ======================================================
elif page == "ğŸ’¬ PhÃ¢n tÃ­ch comment":
    st.title("ğŸ’¬ PhÃ¢n tÃ­ch comment")

    model = joblib.load(MODEL_DIR / "sentiment_model.joblib")

    comments = st.text_area(
        "âœï¸ Nháº­p comment (má»—i dÃ²ng = 1 comment)",
        height=200,
        value=st.session_state.comments_input
    )

    if st.button("ğŸ” PhÃ¢n tÃ­ch"):
        st.session_state.comments_input = comments

        rows = [c for c in comments.split("\n") if c.strip()]
        clean = [clean_text_inference(c) for c in rows]

        probs = model.predict_proba(clean)
        preds = model.predict(clean)
        confs = calibrated_confidence(
            np.max(probs, axis=1)
        ) * 100

        st.session_state.comment_results = list(zip(rows, preds, confs))

    for i, (raw, pred, conf) in enumerate(st.session_state.comment_results, 1):
        lvl = confidence_level(conf)
        st.markdown(f"### ğŸ’¬ Comment {i}")
        st.write(raw)

        c1, c2, c3 = st.columns(3)
        with c1:
            st.metric("Sentiment", pred)
        with c2:
            st.metric("Äá»™ tin cáº­y", f"{confidence_icon(lvl)} {lvl}")
        with c3:
            st.metric("XÃ¡c suáº¥t", f"{conf:.1f}%")

        st.info(explain_comment(pred, conf))
        st.divider()

# ======================================================
# ğŸ›’ PRODUCT ANALYSIS (GIá»® NGUYÃŠN)
# ======================================================
elif page == "ğŸ›’ PhÃ¢n tÃ­ch theo sáº£n pháº©m":
    st.title("ğŸ›’ PhÃ¢n tÃ­ch pháº£n há»“i theo sáº£n pháº©m")

    model = joblib.load(MODEL_DIR / "sentiment_model.joblib")
    col = st.session_state.TEXT_COL
    keyword = st.text_input("ğŸ” Tá»« khÃ³a sáº£n pháº©m")

    if st.button("ğŸ“Š PhÃ¢n tÃ­ch") and col:
        dfp = df[
            df[col].astype(str).str.contains(keyword, case=False, na=False)
        ].copy()

        dfp["sentiment"] = model.predict(
            dfp[col].apply(clean_text_inference)
        )

        st.bar_chart(dfp["sentiment"].value_counts())
        st.dataframe(dfp[[col, "sentiment"]].head(50))

# ======================================================
# ğŸ§© TOPIC MODELING (CÃ“ NÃšT CHáº Y)
# ======================================================
elif page == "ğŸ§© Topic Modeling":
    st.title("ğŸ§© Topic Modeling")

    col = st.session_state.TEXT_COL
    if col is None:
        st.warning("Cáº§n huáº¥n luyá»‡n model trÆ°á»›c")
        st.stop()

    algo = st.selectbox("Thuáº­t toÃ¡n", ["LDA", "NMF"])
    n_topics = st.slider("Sá»‘ topic", 2, 12, 5)

    if st.button("â–¶ï¸ Cháº¡y"):
        texts = df[col].astype(str).apply(clean_text_topic)

        if algo == "LDA":
            vec = CountVectorizer(min_df=5)
            X = vec.fit_transform(texts)
            tm = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42
            )
        else:
            vec = TfidfVectorizer(min_df=5)
            X = vec.fit_transform(texts)
            tm = NMF(
                n_components=n_topics,
                random_state=42
            )

        tm.fit(X)
        words = vec.get_feature_names_out()

        topics = []
        for topic in tm.components_:
            top = topic.argsort()[::-1][:10]
            topics.append(
                [(words[i], topic[i]) for i in top]
            )

        st.session_state.topic_result = topics

    if st.session_state.topic_result:
        for i, topic in enumerate(st.session_state.topic_result, 1):
            st.subheader(f"ğŸ“Œ Topic {i}")
            st.dataframe(
                pd.DataFrame(topic, columns=["Word", "Importance"])
            )
